\section{University of Hawai'i Cluster}

\subsection{{\craycs}}
\begin{frame}
\frametitle{{\craycs} -- History}
\begin{itemize}
 \item Initial investment of 1.8 Million by the University of {\hawaii} (UH)
 \item Delivered in October of 2014
 \item Accepted in December of 2014 
 \item Early adopter testing started in January 2015
 \item Opened for general use in April 2015
 \item March 2016, 92 new nodes added to cluster, raising total core count from 3,800 to 5,876 cores
 \item January 2017, 5 new nodes added to cluster, raising total core count from 5,876 to 5,992 cores
 \item As of Nov 2016, more than 300 users have been granted access to the cluster
\end{itemize}
\end{frame}


\subsubsection{Hardware}
\begin{frame}
	\frametitle{{\craycs} -- Compute Nodes}
	\begin{itemize}
        \item All nodes are diskless with some RAM used for the Operating System
	\item 178 Standard nodes {\tiny(\emph{community})}
	  \begin{itemize}
            {\footnotesize
	    \item Two 10 core {\intel} Ivy-Bridge processors (\emph{20 cores total})
	    \item 128GB of physical RAM with $\approx118$GB of useable RAM
            }
	  \end{itemize}          
	\item 6 Large memory nodes {\tiny(\emph{community})}
	  \begin{itemize}
            {\footnotesize
	    \item Four 10 core {\intel}  Ivy-Bridge processors (\emph{40 cores total})
	    \item 1TB of physical RAM with $\approx1000$GB of useable RAM
            }
	  \end{itemize}			
        \item 1 GPU node {\tiny(\emph{community})}
          \begin{itemize}
            {\footnotesize
	    \item Two 10 core {\intel} Haswell processors (\emph{20 cores total})
	    \item 128GB of physical RAM with $\approx118$GB of useable RAM
            \item 2 Nvidia Tesla K40 GPUs
            }
	  \end{itemize}
        \item 96 Owner nodes
          \begin{itemize}
            {\footnotesize
            \item 33 nodes have two 10 core {\intel} Haswell processors with 256 GB of RAM
            \item 62 nodes have two 12 core {\intel} Haswell processors with 128 GB of RAM
            \item 1 GPU node
            }
          \end{itemize}
	\item CentOS Linux
	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{{\craycs} -- Storage}
	\begin{enumerate}
		\item {\lustre}
                  \begin{itemize}
                  \item High performance parallel filesystem 
                  \item Available on all the cluster nodes
                    \item Freely available to all users
                  \end{itemize}
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{{\craycs} -- Storage $\rightarrow$ {\lustre}}
	\begin{itemize}
		\item The {\craycs} has $\approx582$TB of storage space

		\item Primarily used as scratch space for jobs (Input and Output)
		\item User do not have a usage quota (soft or hard)
		\item Certain directories are subject to a 90 day purge policy
		\item \textbf{Data is not backed up!  Users are responsible for their own data}
                \item {\lustre} utilizes RAID 6 arrays, and is fairly robust but \ldots~\\RAID is not a backup
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{{\craycs} -- Network}
	\begin{itemize}
		\item 40Gb Infiniband inter-connects (QDR)
		\begin{itemize}
			\item High speed inter-connect between~\\compute nodes, {\lustre} storage and Login nodes
                        \item low latency ($\approx1.3\mu$s)
			\item Utilizes the \emph{fat tree network topology}
			\item[] \includegraphics[width=0.20\textwidth]{images/Fat_tree_network} \\[-1ex] {\fontsize{3}{4} \selectfont Source: \url{https://en.wikipedia.org/wiki/Fat_tree} } 		
		\end{itemize}
		\item 10Gb login node internet connectivity
		\begin{itemize}
			\item Speed test from UH to CERN clocked transfer speeds up to 2$+$ Gb/s
		\end{itemize}	
	\end{itemize}
\end{frame}

\subsubsection{Layout}
\begin{frame}
	\frametitle{{\craycs} -- Layout}
	\includegraphics[width=1\textwidth]{images/layout}
\end{frame}

